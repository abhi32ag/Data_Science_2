{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Problem Set 4, due '05/23/2017' at 3:30pm.\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions.\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
    "* Part 4 (especially 3.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](http://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.17.1.\n"
     ]
    }
   ],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "bdata = load_boston()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "Use different learning rates\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between  median housing price and number of rooms per house. Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: ['data', 'feature_names', 'DESCR', 'target']\n",
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uncomment the following if you want to see a lengthy description of the dataset\n",
    "print \"keys:\", bdata.keys()\n",
    "# print \"feature names:\",  bdata.feature_names\n",
    "# print \"data shape:\", bdata.data.shape\n",
    "# print \"target shape\", bdata.target.shape\n",
    "print bdata.DESCR\n",
    "# print \"-\"*80\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bdata.data\n",
    "Y = bdata.target \n",
    "X_df = pd.DataFrame(X, columns = bdata.feature_names)\n",
    "Y_df = pd.DataFrame(Y, columns = ['MEDV'])\n",
    "df = pd.concat([X_df, Y_df], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept   -34.670621\n",
      "RM            9.102109\n",
      "dtype: float64\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.484\n",
      "Model:                            OLS   Adj. R-squared:                  0.483\n",
      "Method:                 Least Squares   F-statistic:                     471.8\n",
      "Date:                Sat, 13 May 2017   Prob (F-statistic):           2.49e-74\n",
      "Time:                        13:43:24   Log-Likelihood:                -1673.1\n",
      "No. Observations:                 506   AIC:                             3350.\n",
      "Df Residuals:                     504   BIC:                             3359.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -34.6706      2.650    -13.084      0.000       -39.877   -29.465\n",
      "RM             9.1021      0.419     21.722      0.000         8.279     9.925\n",
      "==============================================================================\n",
      "Omnibus:                      102.585   Durbin-Watson:                   0.684\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              612.449\n",
      "Skew:                           0.726   Prob(JB):                    1.02e-133\n",
      "Kurtosis:                       8.190   Cond. No.                         58.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "result1 = sm.ols(formula=\"MEDV ~ RM\", data=df, missing = 'drop').fit()\n",
    "print result1.params\n",
    "print result1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM    -22.643262\n",
      "RM2     2.470124\n",
      "dtype: float64\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.548\n",
      "Model:                            OLS   Adj. R-squared:                  0.547\n",
      "Method:                 Least Squares   F-statistic:                     305.4\n",
      "Date:                Sat, 13 May 2017   Prob (F-statistic):           1.46e-87\n",
      "Time:                        13:43:26   Log-Likelihood:                -1639.1\n",
      "No. Observations:                 506   AIC:                             3284.\n",
      "Df Residuals:                     503   BIC:                             3297.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     66.0588     12.104      5.458      0.000        42.278    89.839\n",
      "RM           -22.6433      3.754     -6.031      0.000       -30.019   -15.267\n",
      "RM2            2.4701      0.291      8.502      0.000         1.899     3.041\n",
      "==============================================================================\n",
      "Omnibus:                       82.173   Durbin-Watson:                   0.689\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              934.337\n",
      "Skew:                           0.224   Prob(JB):                    1.29e-203\n",
      "Kurtosis:                       9.642   Cond. No.                     1.91e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.91e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "df['RM2'] = df['RM']**2\n",
    "result2 = sm.ols(formula=\"MEDV ~ RM + RM2\", data=df, missing = 'drop').fit()\n",
    "print result2.params[1:]\n",
    "print result2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when we utilize just the average number of room dwellings in the regression, we obtain an r-sqaured of 0.484 and the estimate for RM is 9.102 which means that with an increase in the room number by 1 there is an increase in the median house value by 9k. Let's take a look at the second model where we regress median value of home over RM + RM-Squared. Here we see that the estimate for RM becomes negative since we are considering the squared of the average number of rooms. So now the estimate for RM2 is understood as for increase in the squared number of rooms by 1 we have an increase in the median house value by 2.4k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 250-fold cross-validation to fit regression (a) above, i.e. the linear fit of housing price on number of rooms per house.\n",
    "Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the 250 slope coefficients, and draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you notice?\n",
    "\n",
    "Note: please use 'random_state=1' and keep 'shuffle=False' when doing the KFold splitting to ensure reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "indices are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89958ebe1c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MEDV ~ RM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mslope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mslope_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, convert, is_copy)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   1627\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m                                    convert=True, verify=True)\n\u001b[0m\u001b[1;32m   1629\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   3635\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3637\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds"
     ]
    }
   ],
   "source": [
    "kf = KFold(len(X_df),n_folds = 250, random_state = 1, shuffle = False)\n",
    "slope_list = []\n",
    "import statsmodels.formula.api as sm\n",
    "for train,test in kf:\n",
    "    result = sm.ols(formula=\"MEDV ~ RM\", data=df[train], missing = 'drop').fit()\n",
    "    slope = result.params[1]\n",
    "    slope_list.append(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 250, random_state = 1, shuffle = False)\n",
    "list_of_slopes\n",
    "for train,test in kf.split(X_df):\n",
    "    sresult = sm.ols(formula=\"MEDV ~ RM \", data=df, missing = 'drop').fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the two regression lines from 1.1 (or 1.2 if you prefer to do so). Show the linear regression line in red, and the linear+quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['RM + RM2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01ddb251c819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MEDV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RM + RM2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MEDV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/seaborn/linearmodels.pyc\u001b[0m in \u001b[0;36mlmplot\u001b[0;34m(x, y, data, hue, col, row, palette, col_wrap, size, aspect, markers, sharex, sharey, hue_order, col_order, row_order, legend, legend_out, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, x_jitter, y_jitter, scatter_kws, line_kws)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0mneed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_partial\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneed_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;31m# Initialize the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2031\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/gl-env/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1208\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['RM + RM2'] not in index\""
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAFdCAYAAAC+UqTcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqIa8v8Dv5Sebf27jZgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116c22c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns; sns.set(color_codes = True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = 'RM', y = 'MEDV', data = df, ci = 95, aspect =2.5)\n",
    "sns.lmplot(x = 'RM + RM2', y = 'MEDV', data=df, ci = 95, aspect = 2.5, line_kws={'color': 'red'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (Average rooms per house)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: It is always useful to include maximum number of iterations, otherwise a rogue estimation may go on forever.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    return np.dot(feature_matrix, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    errors = np.squeeze(np.asarray(errors))\n",
    "    feature = np.squeeze(np.asarray(feature))\n",
    "    return 2*np.dot(errors, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, output):\n",
    "    return np.sqrt(np.mean((predictions-output)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(feature_matrix, output, initial_weights, R, tolerance, max_iter = 100000):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    bivariate_ols: Gradient Decent to minimize OLS. Used to find coefficients of bivariate OLS Linear regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xvalues, yvalues : narray xvalues: independent variable; yvalues: dependent variable\n",
    "    R: float, Learning rate\n",
    "    MaxIterations: Int, maximum number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha: float, intercept\n",
    "    beta: float, co-efficient\n",
    "    ttaken: time taken\n",
    "    it: number of iterations\n",
    "    convergence: logical, convergence achieved?\n",
    "    \"\"\"\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights)\n",
    "    count = 0 \n",
    "    start_time = timeit.default_timer()\n",
    "    while not converged and count < max_iter:\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        \n",
    "        \n",
    "        errors = predictions - output \n",
    "       \n",
    "        gradient_sum_squares = 0\n",
    "        for i in xrange(len(weights)):\n",
    "            gradient = feature_derivative(errors, feature_matrix[:,i])\n",
    "            gradient_sum_squares += gradient**2\n",
    "            weights[i] = weights[i] - R*gradient\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(gradient_sum_squares)\n",
    "        \n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True \n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "        \n",
    "    return weights, elapsed, count, converged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_df['constant'] = 1 \n",
    "features = ['constant','RM']\n",
    "feature_df = X_df[features]\n",
    "feature_matrix = feature_df.as_matrix()\n",
    "output = Y_df.as_matrix()\n",
    "initial_weights = [0. for i in range(len(feature_matrix[0]))]\n",
    "initial_weights = np.array(initial_weights).reshape(-1,1)\n",
    "step_size = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149596.333265\n",
      "36514.5285909\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "weights, time_taken, iterations, converged = gradient_descent(feature_matrix, output, initial_weights, step_size,tolerance = 2, max_iter=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights : [[-34.50831023]\n",
      " [  9.07659294]]\n",
      "Time Taken (in seconds):1.61801195145\n",
      "Iterations needed 14736\n",
      "Converged (T/F): True\n"
     ]
    }
   ],
   "source": [
    "print 'Weights : {0}'.format(weights)\n",
    "print 'Time Taken (in seconds):{0}'.format(time_taken)\n",
    "print 'Iterations needed {0}'.format(iterations)\n",
    "print 'Converged (T/F): {0}'.format(converged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6030959676516652"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = predict_output(feature_matrix, weights)\n",
    "compute_rmse(test_predictions,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    \"\"\"\n",
    "    standardizes raw data and returns\n",
    "    standardized data, mean by column, sd by column\n",
    "    \"\"\"\n",
    "    mu = np.mean(raw_data, axis = 0)\n",
    "    sd = np.std(raw_data, axis = 0)\n",
    "    return (raw_data - mu) / sd, mu, sd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using CRIM and RM as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with the following values of R: 0.1, 0.01, and 0.001.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "For step_size : 0.1\n",
      "------------------------------\n",
      "Weights : [[ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "Time Taken (in seconds):16.7606601715\n",
      "Iterations needed 100000\n",
      "Converged (T/F): False\n",
      "RMSE : nan\n",
      "For step_size : 0.01\n",
      "------------------------------\n",
      "Weights : [[ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "Time Taken (in seconds):14.9935870171\n",
      "Iterations needed 100000\n",
      "Converged (T/F): False\n",
      "RMSE : nan\n",
      "For step_size : 0.001\n",
      "------------------------------\n",
      "Weights : [[ 22.53280632]\n",
      " [ -2.24860926]\n",
      " [  5.89441657]]\n",
      "Time Taken (in seconds):0.00164103507996\n",
      "Iterations needed 18\n",
      "Converged (T/F): True\n",
      "RMSE : 6.22807325025\n"
     ]
    }
   ],
   "source": [
    "data = X_df[['CRIM','RM']].as_matrix()\n",
    "std_data, mean, sd = standardize(data)\n",
    "constant_col = np.array([1 for i in range(len(data))]).reshape(-1,1)\n",
    "\n",
    "std_data = np.append(constant_col, std_data,1)\n",
    "step_list = [0.1,0.01,0.001]\n",
    "initial_weights = [0. for i in range(len(std_data[0]))]\n",
    "print initial_weights\n",
    "initial_weights = np.array(initial_weights).reshape(-1,1)\n",
    "for step_size in step_list:\n",
    "    weights, time_taken, iteration, converged = gradient_descent(std_data, output, initial_weights, step_size,tolerance = 2e-7)\n",
    "    print 'For step_size : {0}'.format(step_size)\n",
    "    print '------------------------------'\n",
    "    print 'Weights : {0}'.format(weights)\n",
    "    print 'Time Taken (in seconds):{0}'.format(time_taken)\n",
    "    print 'Iterations needed {0}'.format(iteration)\n",
    "    print 'Converged (T/F): {0}'.format(converged)\n",
    "    test_predictions = predict_output(std_data, weights)\n",
    "    rmse = compute_rmse(test_predictions,output)\n",
    "    print 'RMSE : {0}'.format(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "For step_size : 0.1\n",
      "------------------------------\n",
      "Weights : [[ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "Time Taken (in seconds): 15.5306999683\n",
      "Iterations needed 100000\n",
      "Converged (T/F): False\n",
      "RMSE : nan\n",
      "For step_size : 0.01\n",
      "------------------------------\n",
      "Weights : [[ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "Time Taken (in seconds): 15.4249379635\n",
      "Iterations needed 100000\n",
      "Converged (T/F): False\n",
      "RMSE : nan\n",
      "For step_size : 0.001\n",
      "------------------------------\n",
      "Weights : [[ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "Time Taken (in seconds): 15.6460180283\n",
      "Iterations needed 100000\n",
      "Converged (T/F): False\n",
      "RMSE : nan\n"
     ]
    }
   ],
   "source": [
    "data = X_df[['CRIM','RM']].as_matrix()\n",
    "constant_col = np.array([1 for i in range(len(data))]).reshape(-1,1)\n",
    "\n",
    "data = np.append(constant_col, data,1)\n",
    "step_list = [0.1,0.01,0.001]\n",
    "initial_weights = [0. for i in range(len(data[0]))]\n",
    "print initial_weights\n",
    "initial_weights = np.array(initial_weights).reshape(-1,1)\n",
    "for step_size in step_list:\n",
    "    weights, time_taken, iteration, converged = gradient_descent(data, output, initial_weights, step_size,tolerance = 2e-7)\n",
    "    print 'For step_size : {0}'.format(step_size)\n",
    "    print '------------------------------'\n",
    "    print 'Weights : {0}'.format(weights)\n",
    "    print 'Time Taken (in seconds): {0}'.format(time_taken)\n",
    "    print 'Iterations needed {0}'.format(iteration)\n",
    "    print 'Converged (T/F): {0}'.format(converged)\n",
    "    test_predictions = predict_output(data, weights)\n",
    "    rmse = compute_rmse(test_predictions,output)\n",
    "    print 'RMSE : {0}'.format(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Since the focus is now on prediction rather than the interpretation of the coefficients, first standardize your features before proceeding.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again. Let's fix that in one of two ways. If you're feeling confident, use k-fold cross-validation to re-fit the multiple regression from 2.3 above, and report your estimated coefficients (there should be three, corresponding to the intercept and the two coefficients for CRIM and RM). Or if you want to do the quick and dirty version, randomly divide your data into a training set (66%) and testing set (34%) and use the training set to re-fit the regression from 2.3 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# using k-fold cross validation code from Problem Set 3 \n",
    "def k_cross_validation(data, features, initial_weights, R, tolerance, folds = 10, verbose=False):\n",
    "    if features == []:\n",
    "        data['constant'] = 1\n",
    "        features = ['constant'] + list(data.columns) + ['MEDV']\n",
    "        total_data = data[features]\n",
    "        len_features = total_data.shape[1] -1\n",
    "    else:\n",
    "        len_features = len(features) + 1\n",
    "        data['constant'] = 1\n",
    "        features = ['constant'] + features + ['MEDV']\n",
    "        total_data = data[features]\n",
    "    \n",
    "    \n",
    "    # creating index slices \n",
    "    total_indices = list(total_data.index)\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    np.random.shuffle(total_indices)\n",
    "    \n",
    "    remaining_k = []\n",
    "    size_partition = len(total_indices)/folds\n",
    "    rmse_list = []\n",
    "    \n",
    "    \n",
    "    for i in range(1,folds+1):\n",
    "        if verbose == True:\n",
    "            print 'Iteration :{0}'.format(i)\n",
    "            print '----------------------------'\n",
    "        test = total_indices[(i-1)*size_partition:i*size_partition]\n",
    "        \n",
    "        test1_data = total_data.iloc[test].as_matrix()\n",
    "        test_data,mu, sd = standardize(test1_data[:,1:len_features])\n",
    "        \n",
    "        constant_col = np.array([1 for j in range(len(test1_data))]).reshape(-1,1)\n",
    "        \n",
    "        test_data = np.append(constant_col,test_data, 1)\n",
    "        \n",
    "        test_output = test1_data[:,len_features]\n",
    "        \n",
    "        if verbose == True:\n",
    "            print 'Test data index {0}, {1}'.format((i-1)*size_partition,i*size_partition)\n",
    "            print 'Test data size :{0}'.format(test_data.shape)\n",
    "        remaining_k += test\n",
    "        rem = list(set(test)^set(remaining_k))\n",
    "       \n",
    "        train = rem + total_indices[i*size_partition:]\n",
    "        \n",
    "        train1_data = total_data.iloc[train].as_matrix()\n",
    "        \n",
    "        train_std_data,mu, sd = standardize(train1_data[:,1:len_features])\n",
    "        \n",
    "        constant_col = np.array([1 for j in range(len(train_std_data))]).reshape(-1,1)\n",
    "        \n",
    "        train_data = np.append(constant_col,train_std_data, 1)\n",
    "        \n",
    "        train_output = train1_data[:,len_features]\n",
    "        \n",
    "        if verbose == True:\n",
    "            print 'Train data size :{0}'.format(train_data.shape)\n",
    "        # calculate weights using the training data \n",
    "        \n",
    "        weights, time_taken, iterations, converged = gradient_descent(train_data, train_output, initial_weights, R,tolerance)\n",
    "        \n",
    "        predictions = predict_output(test_data,weights)\n",
    "        print weights\n",
    "        print iterations \n",
    "         \n",
    "        pred = np.array(predictions)\n",
    "        rmse = compute_rmse(pred,test_output)\n",
    "        if verbose == True:\n",
    "            print 'RMSE :{0}'.format(rmse)\n",
    "            print ' '\n",
    "        rmse_list.append(rmse)\n",
    "    return rmse_list, predictions, test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.53662281  -2.20775313   5.66693171]\n",
      "18\n",
      "[ 22.56315789  -2.15439924   5.91251261]\n",
      "16\n",
      "[ 22.58223684  -2.3482798    5.88996288]\n",
      "16\n",
      "[ 22.45526316  -2.29952735   5.90164635]\n",
      "16\n",
      "[ 22.53574561  -2.42882689   5.83031776]\n",
      "16\n",
      "[ 22.6997807   -2.11606282   5.96905483]\n",
      "16\n",
      "[ 22.67127193  -2.28318455   5.96228079]\n",
      "16\n",
      "[ 22.17565789  -2.37163984   5.67846343]\n",
      "16\n",
      "[ 22.3375      -2.21834354   6.03094758]\n",
      "16\n",
      "[ 22.79714912  -2.05867894   6.12318956]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import timeit\n",
    "initial_weights = [0. for i in range(3)]\n",
    "list1, predictions, output = k_cross_validation(df, features = ['CRIM','RM'],initial_weights = initial_weights, R = 0.001, tolerance = 2e-5, folds = 10, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2518306308287963"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Compute the RMSE on your test cases, i.e. take the model parameters that you found above and compare the actual to the predicted values for just the test instances. If you did this the k-fold way above, this will be the average RMSE across the k test sets. If you did this the quick and dirty way above, this will just be the RMSE on your single test set.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?  How does it compare to RMSE from a simpler model where number of rooms is the only explanatory variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Extra Credit 1: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $40,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: Create new interaction variables between each possible pair of the F_s features.  Note: as all the original features are numeric, you can just multiply.  If you originally had *K* features, you should now have K+(K*(K+1))/2 features. Standardize all of your features.\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# create interaction terms \n",
    "# from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 4.2 Let's overfit!\n",
    "Now, using your version of multiple regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those K+(K*(K+1))/2 features as you can.  If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 4.3 Ridge regularization\n",
    "a) Incorporate L2 (Ridge) regularization into your multiple_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "b) Use your regularized regression to re-fit the model from 4.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "c) Go brag to your friends about how you just implemented ridge-regularized multiple regression using gradient descent optimization, from scratch (if you still have any friends left...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Extra Credit 2: Cross-validate lambda\n",
    "\n",
    "Use k-fold cross-validation to select the optimal value of lambda. Report the average RMSE across all training sets, and the average RMSE across all testing sets. How do these numbers compare to each other, to the RMSE from your previous efforts?  Finally, create a plot that shows RMSE as a function of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (Showoff) Extra Credit 3: Lambda and coefficients\n",
    "\n",
    "If you're feeling extra-special, create a parameter plot that shows how the different coefficient estimates change as a function of lambda. To make this graph intelligible, only include the *K* original F_s features in this plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "INFX574-PS4-solutions.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
